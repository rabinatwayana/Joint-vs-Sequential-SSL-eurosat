{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0cb307d",
   "metadata": {},
   "source": [
    "## Self Supervised Learning (SSL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3266bf3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T10:15:14.856356Z",
     "iopub.status.busy": "2026-02-14T10:15:14.855529Z",
     "iopub.status.idle": "2026-02-14T10:15:24.273061Z",
     "shell.execute_reply": "2026-02-14T10:15:24.272245Z",
     "shell.execute_reply.started": "2026-02-14T10:15:14.856332Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torchgeo --quiet\n",
    "!pip install lightning --quiet\n",
    "!pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce313a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T10:15:24.275591Z",
     "iopub.status.busy": "2026-02-14T10:15:24.275360Z",
     "iopub.status.idle": "2026-02-14T10:15:24.280132Z",
     "shell.execute_reply": "2026-02-14T10:15:24.279191Z",
     "shell.execute_reply.started": "2026-02-14T10:15:24.275566Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72eb59f-1500-4e39-b46c-556d5a33e05f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T10:15:24.281477Z",
     "iopub.status.busy": "2026-02-14T10:15:24.281239Z",
     "iopub.status.idle": "2026-02-14T10:15:24.301660Z",
     "shell.execute_reply": "2026-02-14T10:15:24.300980Z",
     "shell.execute_reply.started": "2026-02-14T10:15:24.281445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "print(\"CPU Count: \", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90794fb4-bc3f-48cf-a840-cbe31a22fcd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T10:15:24.302951Z",
     "iopub.status.busy": "2026-02-14T10:15:24.302757Z",
     "iopub.status.idle": "2026-02-14T10:15:24.316373Z",
     "shell.execute_reply": "2026-02-14T10:15:24.315688Z",
     "shell.execute_reply.started": "2026-02-14T10:15:24.302930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import rasterio\n",
    "import pytorch_lightning as pl\n",
    "from torchvision.models import resnet50\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch import Trainer\n",
    "from torchvision import transforms  \n",
    "from torchgeo.trainers.moco import MoCoTask\n",
    "from torchgeo.models import ResNet18_Weights\n",
    "import kornia.augmentation as K\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import glob\n",
    "import shutil\n",
    "import random\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f7783-8984-445f-86c3-c0d095d433fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T10:15:24.317460Z",
     "iopub.status.busy": "2026-02-14T10:15:24.317206Z",
     "iopub.status.idle": "2026-02-14T10:15:24.337415Z",
     "shell.execute_reply": "2026-02-14T10:15:24.336410Z",
     "shell.execute_reply.started": "2026-02-14T10:15:24.317426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pl.seed_everything(SEED, workers=True)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795ed230-3932-4ff3-bb39-e13f7b081e2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T10:46:10.337008Z",
     "iopub.status.busy": "2026-02-14T10:46:10.336701Z",
     "iopub.status.idle": "2026-02-14T10:46:10.573656Z",
     "shell.execute_reply": "2026-02-14T10:46:10.572908Z",
     "shell.execute_reply.started": "2026-02-14T10:46:10.336973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_dataset(root_dir,csv_output)\n",
    "    # --- COLLECT IMAGE INFO ---\n",
    "    data=[]\n",
    "    # Iterate over each class folder\n",
    "    for class_name in os.listdir(root_dir):\n",
    "        class_path = os.path.join(root_dir, class_name)\n",
    "        # print(class_path)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue  # skip files in root_dir\n",
    "    \n",
    "        # Iterate over images in the class folder\n",
    "        for fname in os.listdir(class_path):\n",
    "            # if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            if fname.lower().endswith((\".tif\", \".tiff\")):\n",
    "                # path = os.path.join(class_path, fname)\n",
    "                rel_path = os.path.join(class_name, fname)\n",
    "                data.append({\n",
    "                    \"id\": os.path.splitext(fname)[0].split(\"_\")[-1],\n",
    "                     \"fname\": fname,\n",
    "                    \"rel_path\": rel_path,\n",
    "                    \"label\": class_name\n",
    "                })\n",
    "    # --- CREATE DATAFRAME ---\n",
    "    df = pd.DataFrame(data)\n",
    "    # print(df.columns)\n",
    "    # --- STRATIFIED SPLIT: 80% SSL, 20% Downstream ---\n",
    "    ssl_df, downstream_df = train_test_split(\n",
    "        df,\n",
    "        test_size=0.2,\n",
    "        stratify=df['label'],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    ssl_df['task'] = 'ssl'\n",
    "    downstream_df['task'] = 'downstream'\n",
    "    \n",
    "    df = pd.concat([ssl_df, downstream_df]).reset_index(drop=True)\n",
    "    \n",
    "    ssl_df = df[df['task'] == 'ssl'].copy()\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    \n",
    "    ssl_splits = np.empty(len(ssl_df), dtype=object)\n",
    "    split_names = ['subset1', 'subset2', 'subset3', 'subset4']\n",
    "    \n",
    "    for fold_idx, (_, val_idx) in enumerate(skf.split(ssl_df, ssl_df['label'])):\n",
    "        ssl_splits[val_idx] = split_names[fold_idx]\n",
    "    \n",
    "    df.loc[ssl_df.index, 'split'] = ssl_splits\n",
    "    \n",
    "    down_df = df[df['task'] == 'downstream'].copy()\n",
    "    # Step 1: Train (70%) vs Temp (30%)\n",
    "    train_df, temp_df = train_test_split(\n",
    "        down_df,\n",
    "        test_size=0.30,\n",
    "        stratify=down_df['label'],\n",
    "        random_state=42\n",
    "    )\n",
    "    # Step 2: Temp â†’ Val (15%) + Test (15%)\n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df,\n",
    "        test_size=0.50,  # half of 30% = 15%\n",
    "        stratify=temp_df['label'],\n",
    "        random_state=42\n",
    "    )\n",
    "    # Assign splits back\n",
    "    df.loc[train_df.index, 'split'] = 'train'\n",
    "    df.loc[val_df.index, 'split'] = 'val'\n",
    "    df.loc[test_df.index, 'split'] = 'test'\n",
    "    \n",
    "    # --- FINAL CHECK ---\n",
    "    print(df['task'].value_counts())\n",
    "    print(df['split'].value_counts())\n",
    "    # print(df.head(10))\n",
    "    \n",
    "    # --- SAVE CSV ---\n",
    "    df.to_csv(csv_output, index=False)\n",
    "    print(f\"CSV saved to {csv_output}\")\n",
    "\n",
    "class SSLDataset(Dataset):\n",
    "    def __init__(self, data_dir, split_path,split, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (str): Eurosat folder paths.\n",
    "            split_path (str): CSV file path containing splits metadata\n",
    "            split (str): all, full_ssl, subset1, subset2, subset3, subset4, train, test, val\n",
    "            transforms (callable, optional): Optional transform to apply to patches.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.split_path = split_path\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # Precompute all paths based on split\n",
    "        self.samples = []\n",
    "        df=pd.read_csv(split_path)\n",
    "        if split==\"all\":\n",
    "            pass\n",
    "        elif split==\"full_ssl\":\n",
    "            df=df[df['task'] == \"ssl\"]\n",
    "        else:\n",
    "            df=df[df['split'] == split]\n",
    "        self.samples = df['rel_path'].tolist()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     sample_path = os.path.join(self.data_dir, self.samples[idx])\n",
    "        \n",
    "    #     with Image.open(sample_path) as img:\n",
    "    #         patch_tensor = img.convert(\"RGB\")\n",
    "    #     if self.transforms:\n",
    "    #         patch_tensor = self.transforms(patch_tensor)\n",
    "\n",
    "    #     return {\"image\": patch_tensor}\n",
    "    def __getitem__(self, idx):\n",
    "        sample_path = os.path.join(self.data_dir, self.samples[idx])\n",
    "    \n",
    "        # --- Read TIFF with rasterio ---\n",
    "        with rasterio.open(sample_path) as src:\n",
    "            # Read all bands as float32\n",
    "            bands = [src.read(b).astype(np.float32) for b in range(1, src.count + 1)]\n",
    "        \n",
    "        # Stack bands to shape [C, H, W]\n",
    "        img_array = np.stack(bands, axis=0)\n",
    "    \n",
    "        # --- Convert to torch tensor ---\n",
    "        patch_tensor = torch.tensor(img_array, dtype=torch.float32)\n",
    "    \n",
    "        # --- Apply transforms if provided ---\n",
    "        if self.transforms:\n",
    "            patch_tensor = self.transforms(patch_tensor)\n",
    "    \n",
    "        return {\"image\": patch_tensor}\n",
    "\n",
    "\n",
    "def calculate_stats(dataset, n_samples=500):\n",
    "    mean = 0\n",
    "    std = 0\n",
    "    total = len(dataset)\n",
    "    n = min(total, n_samples)\n",
    "\n",
    "    # Randomly choose n indices\n",
    "    np.random.seed(42)\n",
    "    indices = np.random.choice(total, size=n, replace=False)\n",
    "    # count=0\n",
    "    for i in indices:\n",
    "        # count=count+1\n",
    "        # print(count)\n",
    "        sample = dataset[i]\n",
    "        # print(sample)\n",
    "        img = sample[\"image\"]   # TorchGeo-style dictionary\n",
    "\n",
    "        mean += img.mean(dim=(1, 2))\n",
    "        std += img.std(dim=(1, 2))\n",
    "    mean /= n\n",
    "    std /= n\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def summary_trainable(model):\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Module\", \"Type\", \"Trainable Params\", \"Total Params\"]\n",
    "\n",
    "    for name, module in model.named_children():\n",
    "        total_params = sum(p.numel() for p in module.parameters())\n",
    "        trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "        table.add_row([name, type(module).__name__, f\"{trainable_params:,}\", f\"{total_params:,}\"])\n",
    "\n",
    "    total_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(table)\n",
    "    print(f\"Total trainable parameters: {total_trainable:,} ({total_trainable / 1e6:.2f} M)\")\n",
    "    print(f\"Total parameters: {total_params:,} ({total_params / 1e6:.2f} M)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd6036-66d2-4393-a9ee-3c57cf5740a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T10:46:17.221685Z",
     "iopub.status.busy": "2026-02-14T10:46:17.221383Z",
     "iopub.status.idle": "2026-02-14T10:46:17.230660Z",
     "shell.execute_reply": "2026-02-14T10:46:17.229509Z",
     "shell.execute_reply.started": "2026-02-14T10:46:17.221658Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- CONFIG ---\n",
    "root_dir = \"/kaggle/input/datasets/apollo2506/eurosat-dataset/EuroSATallBands\"  # folder with all images\n",
    "csv_output = \"/kaggle/working/eurosat_all_bands_split.csv\"\n",
    "split_dataset(root_dir, csv_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b09f9d-69a1-48eb-bbf0-50d07bf8eb8e",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89c3361-f4bb-4fd4-9ca2-75066c36d8f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T10:47:34.818310Z",
     "iopub.status.busy": "2026-02-14T10:47:34.818010Z",
     "iopub.status.idle": "2026-02-14T10:47:34.828183Z",
     "shell.execute_reply": "2026-02-14T10:47:34.827386Z",
     "shell.execute_reply.started": "2026-02-14T10:47:34.818284Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "target_size = 224\n",
    "target_batch_size= 8 #128 #prefer 256 or 128\n",
    "target_num_workers=4\n",
    "target_max_epoch=6\n",
    "use_peft = True  \n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "logger = CSVLogger(\"logs\", name=f\"metrics_{timestamp}\")\n",
    "\n",
    "aug = K.AugmentationSequential(\n",
    "    K.RandomResizedCrop(size=(target_size, target_size), scale=(0.4, 1.0)),\n",
    "    K.RandomHorizontalFlip(),\n",
    "    K.RandomVerticalFlip(),\n",
    "    K.RandomGaussianBlur(kernel_size=(7,7), sigma=(0.1, 1.5), p=0.3),\n",
    "    K.RandomBrightness(brightness=(0.85, 1.15), p=0.5),\n",
    "    data_keys=['input'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a4645",
   "metadata": {},
   "source": [
    "### Compute Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5a9dfb-55db-4e1e-baa6-491857d7d83d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T10:48:28.145464Z",
     "iopub.status.busy": "2026-02-14T10:48:28.145196Z",
     "iopub.status.idle": "2026-02-14T10:48:28.192596Z",
     "shell.execute_reply": "2026-02-14T10:48:28.191918Z",
     "shell.execute_reply.started": "2026-02-14T10:48:28.145439Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "temp_dataset = SSLDataset(\n",
    "    data_dir = \"/kaggle/input/datasets/apollo2506/eurosat-dataset/EuroSATallBands\", \n",
    "    split_path= \"/kaggle/working/eurosat_all_bands_split.csv\",\n",
    "    split = \"all\",\n",
    ")\n",
    "mean, std = calculate_stats(temp_dataset, n_samples=50)\n",
    "print(mean, std )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0716eda6-1694-4829-8b66-c6b6fd4369a7",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# based on 10k samples\n",
    "mean= [1333.8029, 1488.1448, 1745.9066, 1985.6210, 2322.0129, 2837.1787,\n",
    "        3065.8462, 3192.4492, 3225.1826, 3344.8479,    0.0000, 2683.2991,\n",
    "        2116.8357]\n",
    "std = [384.9683, 472.5244, 497.7275, 590.9384, 578.0192, 641.7764, 699.6282,\n",
    "        752.0769, 709.3992, 752.4539,   0.0000, 568.3574, 542.2833]\n",
    "\n",
    "\n",
    "# to avoid 0 std\n",
    "std = [max(s, 1e-5) for s in std]   \n",
    "\n",
    "# define transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186dd8fb-e8e9-4fc1-aa47-8da54f22c9fb",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_ssl(\n",
    "    data_dir,\n",
    "    split_path,\n",
    "    split=\"all\",\n",
    "    model=\"resnet18\",\n",
    "    weights=None,\n",
    "    in_channels=13,\n",
    "    mean=[],\n",
    "    std=[],\n",
    "    transform=None,\n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    target_size=224,\n",
    "    lr=1e-4,\n",
    "    memory_bank_size=2048,\n",
    "    temperature=0.15,\n",
    "    use_peft=False,\n",
    "    augmentation1=None,\n",
    "    augmentation2=None,\n",
    "    max_epochs=10,\n",
    "    experiment_name= None,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Run SSL experiment.\n",
    "\n",
    "    Args:\n",
    "        split: \"all\" for full dataset or \"subst1\"/\"subst2\"/... for sequential streaming\n",
    "        weights: ResNet18_Weights object\n",
    "        transform: preprocessing/normalization transform\n",
    "        augmentation1/2: MoCo augmentations\n",
    "        use_peft: bool, freeze backbone except last block\n",
    "        Other args: training hyperparameters\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # Dataset / DataLoader\n",
    "    # -----------------------------\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    logger = CSVLogger(\"logs\", name=f\"{experiment_name}/metrics_{timestamp}\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize((target_size, target_size)),\n",
    "            transforms.Normalize(mean=mean, std=std)\n",
    "        ])\n",
    "    \n",
    "    dataset = SSLDataset(\n",
    "        data_dir=data_dir,\n",
    "        split_path=split_path,\n",
    "        split=split,\n",
    "        transforms=transform\n",
    "    )\n",
    "    print(f\"Dataset split '{split}' size:\", len(dataset))\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=lambda worker_id: np.random.seed(seed + worker_id)\n",
    "    )\n",
    "\n",
    "    num_batches = len(data_loader)\n",
    "    print(\"Number of batches:\", num_batches)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Initialize MoCo task\n",
    "    # -----------------------------\n",
    "    task = MoCoTask(\n",
    "        model=model,\n",
    "        weights=weights,\n",
    "        in_channels= in_channels,#weights.meta['in_chans'] if weights else 3,\n",
    "        version=2,\n",
    "        size=target_size,\n",
    "        augmentation1=augmentation1,\n",
    "        augmentation2=augmentation2,\n",
    "        lr=lr,\n",
    "        memory_bank_size=memory_bank_size,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # PEFT / Full Fine-Tuning Logic\n",
    "    # -----------------------------\n",
    "    if use_peft:\n",
    "        print(\"Using PEFT: freezing backbone except last block, training projection head...\")\n",
    "        for name, param in task.backbone.named_parameters():\n",
    "            param.requires_grad = \"layer4\" in name\n",
    "    else:\n",
    "        print(\"Full fine-tuning: backbone and projection head trainable...\")\n",
    "        for param in task.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # Momentum backbone always frozen\n",
    "    for param in task.backbone_momentum.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Projection head always trainable\n",
    "    for param in task.projection_head.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    summary_trainable(task)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Trainer\n",
    "    # -----------------------------\n",
    "    trainer = Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        enable_progress_bar=True,\n",
    "        log_every_n_steps=num_batches,\n",
    "        precision=32,\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        deterministic=True,\n",
    "        logger=logger\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # Training\n",
    "    # -----------------------------\n",
    "    start_time = time.time()\n",
    "    trainer.fit(task, data_loader)\n",
    "    end_time = time.time()\n",
    "    print(f\"Training time: {(end_time-start_time)/60:.2f} min\")\n",
    "\n",
    "    print(task.trainer.logged_metrics)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save checkpoint / weights\n",
    "    # -----------------------------\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    torch.save(task.backbone.state_dict(), f\"{experiment_name}/checkpoints/ssl_backbone_{timestamp}.pth\")\n",
    "    torch.save(task.projection_head.state_dict(), f\"{experiment_name}/checkpoints/projection_head_{timestamp}.pth\")\n",
    "    trainer.save_checkpoint(f\"{experiment_name}/checkpoints/ssl_ckpt_{timestamp}.ckpt\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 918039,
     "sourceId": 1663377,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "torchgeo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
