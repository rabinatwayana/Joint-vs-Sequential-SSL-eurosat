{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1663377,"sourceType":"datasetVersion","datasetId":918039}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"e0cb307d","cell_type":"markdown","source":"## Self Supervised Learning (SSL)","metadata":{}},{"id":"3266bf3c","cell_type":"code","source":"!pip install torchgeo --quiet\n!pip install lightning --quiet\n!pip install prettytable","metadata":{"execution":{"iopub.execute_input":"2026-02-14T10:15:14.856356Z","iopub.status.busy":"2026-02-14T10:15:14.855529Z","iopub.status.idle":"2026-02-14T10:15:24.273061Z","shell.execute_reply":"2026-02-14T10:15:24.272245Z","shell.execute_reply.started":"2026-02-14T10:15:14.856332Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"fce313a0","cell_type":"code","source":"import os\nSEED = 42\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)","metadata":{"execution":{"iopub.execute_input":"2026-02-14T10:15:24.275591Z","iopub.status.busy":"2026-02-14T10:15:24.275360Z","iopub.status.idle":"2026-02-14T10:15:24.280132Z","shell.execute_reply":"2026-02-14T10:15:24.279191Z","shell.execute_reply.started":"2026-02-14T10:15:24.275566Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"c72eb59f-1500-4e39-b46c-556d5a33e05f","cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\nprint(\"CUDA Available: \", torch.cuda.is_available())\nprint(\"CPU Count: \", os.cpu_count())","metadata":{"execution":{"iopub.execute_input":"2026-02-14T10:15:24.281477Z","iopub.status.busy":"2026-02-14T10:15:24.281239Z","iopub.status.idle":"2026-02-14T10:15:24.301660Z","shell.execute_reply":"2026-02-14T10:15:24.300980Z","shell.execute_reply.started":"2026-02-14T10:15:24.281445Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"90794fb4-bc3f-48cf-a840-cbe31a22fcd7","cell_type":"code","source":"from torch.utils.data import Dataset\nimport rasterio\nimport pytorch_lightning as pl\nfrom torchvision.models import resnet50\nfrom torch.utils.data import DataLoader\nfrom lightning.pytorch import Trainer\nfrom torchvision import transforms  \nfrom torchgeo.trainers.moco import MoCoTask\nfrom torchgeo.models import ResNet18_Weights\nimport kornia.augmentation as K\nfrom lightning.pytorch.loggers import CSVLogger\nimport glob\nimport shutil\nimport random\nfrom prettytable import PrettyTable\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nimport torch\nfrom torchvision import transforms\nimport os\nimport pandas as pd\nimport time\nfrom datetime import datetime","metadata":{"execution":{"iopub.execute_input":"2026-02-14T10:15:24.302951Z","iopub.status.busy":"2026-02-14T10:15:24.302757Z","iopub.status.idle":"2026-02-14T10:15:24.316373Z","shell.execute_reply":"2026-02-14T10:15:24.315688Z","shell.execute_reply.started":"2026-02-14T10:15:24.302930Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"af5f7783-8984-445f-86c3-c0d095d433fe","cell_type":"code","source":"random.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\npl.seed_everything(SEED, workers=True)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\ntorch.use_deterministic_algorithms(True)","metadata":{"execution":{"iopub.execute_input":"2026-02-14T10:15:24.317460Z","iopub.status.busy":"2026-02-14T10:15:24.317206Z","iopub.status.idle":"2026-02-14T10:15:24.337415Z","shell.execute_reply":"2026-02-14T10:15:24.336410Z","shell.execute_reply.started":"2026-02-14T10:15:24.317426Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"795ed230-3932-4ff3-bb39-e13f7b081e2a","cell_type":"code","source":"def split_dataset(root_dir,csv_output)\n    # --- COLLECT IMAGE INFO ---\n    data=[]\n    # Iterate over each class folder\n    for class_name in os.listdir(root_dir):\n        class_path = os.path.join(root_dir, class_name)\n        # print(class_path)\n        if not os.path.isdir(class_path):\n            continue  # skip files in root_dir\n    \n        # Iterate over images in the class folder\n        for fname in os.listdir(class_path):\n            # if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n            if fname.lower().endswith((\".tif\", \".tiff\")):\n                # path = os.path.join(class_path, fname)\n                rel_path = os.path.join(class_name, fname)\n                data.append({\n                    \"id\": os.path.splitext(fname)[0].split(\"_\")[-1],\n                     \"fname\": fname,\n                    \"rel_path\": rel_path,\n                    \"label\": class_name\n                })\n    # --- CREATE DATAFRAME ---\n    df = pd.DataFrame(data)\n    # print(df.columns)\n    # --- STRATIFIED SPLIT: 80% SSL, 20% Downstream ---\n    ssl_df, downstream_df = train_test_split(\n        df,\n        test_size=0.2,\n        stratify=df['label'],\n        random_state=42\n    )\n    \n    ssl_df['task'] = 'ssl'\n    downstream_df['task'] = 'downstream'\n    \n    df = pd.concat([ssl_df, downstream_df]).reset_index(drop=True)\n    \n    ssl_df = df[df['task'] == 'ssl'].copy()\n    \n    skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n    \n    ssl_splits = np.empty(len(ssl_df), dtype=object)\n    split_names = ['subset1', 'subset2', 'subset3', 'subset4']\n    \n    for fold_idx, (_, val_idx) in enumerate(skf.split(ssl_df, ssl_df['label'])):\n        ssl_splits[val_idx] = split_names[fold_idx]\n    \n    df.loc[ssl_df.index, 'split'] = ssl_splits\n    \n    down_df = df[df['task'] == 'downstream'].copy()\n    # Step 1: Train (70%) vs Temp (30%)\n    train_df, temp_df = train_test_split(\n        down_df,\n        test_size=0.30,\n        stratify=down_df['label'],\n        random_state=42\n    )\n    # Step 2: Temp â†’ Val (15%) + Test (15%)\n    val_df, test_df = train_test_split(\n        temp_df,\n        test_size=0.50,  # half of 30% = 15%\n        stratify=temp_df['label'],\n        random_state=42\n    )\n    # Assign splits back\n    df.loc[train_df.index, 'split'] = 'train'\n    df.loc[val_df.index, 'split'] = 'val'\n    df.loc[test_df.index, 'split'] = 'test'\n    \n    # --- FINAL CHECK ---\n    print(df['task'].value_counts())\n    print(df['split'].value_counts())\n    # print(df.head(10))\n    \n    # --- SAVE CSV ---\n    df.to_csv(csv_output, index=False)\n    print(f\"CSV saved to {csv_output}\")\n\nclass SSLDataset(Dataset):\n    def __init__(self, data_dir, split_path,split, transforms=None):\n        \"\"\"\n        Args:\n            data_dir (str): Eurosat folder paths.\n            split_path (str): CSV file path containing splits metadata\n            split (str): all, full_ssl, subset1, subset2, subset3, subset4, train, test, val\n            transforms (callable, optional): Optional transform to apply to patches.\n        \"\"\"\n        self.data_dir = data_dir\n        self.split_path = split_path\n        self.transforms = transforms\n        \n        # Precompute all paths based on split\n        self.samples = []\n        df=pd.read_csv(split_path)\n        if split==\"all\":\n            pass\n        elif split==\"full_ssl\":\n            df=df[df['task'] == \"ssl\"]\n        else:\n            df=df[df['split'] == split]\n        self.samples = df['rel_path'].tolist()\n        \n\n    def __len__(self):\n        return len(self.samples)\n    \n\n    # def __getitem__(self, idx):\n    #     sample_path = os.path.join(self.data_dir, self.samples[idx])\n        \n    #     with Image.open(sample_path) as img:\n    #         patch_tensor = img.convert(\"RGB\")\n    #     if self.transforms:\n    #         patch_tensor = self.transforms(patch_tensor)\n\n    #     return {\"image\": patch_tensor}\n    def __getitem__(self, idx):\n        sample_path = os.path.join(self.data_dir, self.samples[idx])\n    \n        # --- Read TIFF with rasterio ---\n        with rasterio.open(sample_path) as src:\n            # Read all bands as float32\n            bands = [src.read(b).astype(np.float32) for b in range(1, src.count + 1)]\n        \n        # Stack bands to shape [C, H, W]\n        img_array = np.stack(bands, axis=0)\n    \n        # --- Convert to torch tensor ---\n        patch_tensor = torch.tensor(img_array, dtype=torch.float32)\n    \n        # --- Apply transforms if provided ---\n        if self.transforms:\n            patch_tensor = self.transforms(patch_tensor)\n    \n        return {\"image\": patch_tensor}\n\n\ndef calculate_stats(dataset, n_samples=500):\n    mean = 0\n    std = 0\n    total = len(dataset)\n    n = min(total, n_samples)\n\n    # Randomly choose n indices\n    np.random.seed(42)\n    indices = np.random.choice(total, size=n, replace=False)\n    # count=0\n    for i in indices:\n        # count=count+1\n        # print(count)\n        sample = dataset[i]\n        # print(sample)\n        img = sample[\"image\"]   # TorchGeo-style dictionary\n\n        mean += img.mean(dim=(1, 2))\n        std += img.std(dim=(1, 2))\n    mean /= n\n    std /= n\n    return mean, std\n\n\ndef summary_trainable(model):\n    table = PrettyTable()\n    table.field_names = [\"Module\", \"Type\", \"Trainable Params\", \"Total Params\"]\n\n    for name, module in model.named_children():\n        total_params = sum(p.numel() for p in module.parameters())\n        trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n        table.add_row([name, type(module).__name__, f\"{trainable_params:,}\", f\"{total_params:,}\"])\n\n    total_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total_params = sum(p.numel() for p in model.parameters())\n    \n    print(table)\n    print(f\"Total trainable parameters: {total_trainable:,} ({total_trainable / 1e6:.2f} M)\")\n    print(f\"Total parameters: {total_params:,} ({total_params / 1e6:.2f} M)\")","metadata":{"execution":{"iopub.execute_input":"2026-02-14T10:46:10.337008Z","iopub.status.busy":"2026-02-14T10:46:10.336701Z","iopub.status.idle":"2026-02-14T10:46:10.573656Z","shell.execute_reply":"2026-02-14T10:46:10.572908Z","shell.execute_reply.started":"2026-02-14T10:46:10.336973Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"7dbd6036-66d2-4393-a9ee-3c57cf5740a2","cell_type":"code","source":"# --- CONFIG ---\nroot_dir = \"/kaggle/input/datasets/apollo2506/eurosat-dataset/EuroSATallBands\"  # folder with all images\ncsv_output = \"/kaggle/working/eurosat_all_bands_split.csv\"\nsplit_dataset(root_dir, csv_output)","metadata":{"execution":{"iopub.execute_input":"2026-02-14T10:46:17.221685Z","iopub.status.busy":"2026-02-14T10:46:17.221383Z","iopub.status.idle":"2026-02-14T10:46:17.230660Z","shell.execute_reply":"2026-02-14T10:46:17.229509Z","shell.execute_reply.started":"2026-02-14T10:46:17.221658Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"07b09f9d-69a1-48eb-bbf0-50d07bf8eb8e","cell_type":"markdown","source":"### Settings","metadata":{}},{"id":"c89c3361-f4bb-4fd4-9ca2-75066c36d8f3","cell_type":"code","source":"target_size = 224\ntarget_batch_size= 8 #128 #prefer 256 or 128\ntarget_num_workers=4\ntarget_max_epoch=6\nuse_peft = True  \n\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nlogger = CSVLogger(\"logs\", name=f\"metrics_{timestamp}\")\n\naug = K.AugmentationSequential(\n    K.RandomResizedCrop(size=(target_size, target_size), scale=(0.4, 1.0)),\n    K.RandomHorizontalFlip(),\n    K.RandomVerticalFlip(),\n    K.RandomGaussianBlur(kernel_size=(7,7), sigma=(0.1, 1.5), p=0.3),\n    K.RandomBrightness(brightness=(0.85, 1.15), p=0.5),\n    data_keys=['input'],\n)","metadata":{"execution":{"iopub.execute_input":"2026-02-14T10:47:34.818310Z","iopub.status.busy":"2026-02-14T10:47:34.818010Z","iopub.status.idle":"2026-02-14T10:47:34.828183Z","shell.execute_reply":"2026-02-14T10:47:34.827386Z","shell.execute_reply.started":"2026-02-14T10:47:34.818284Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"c11a4645","cell_type":"markdown","source":"### Compute Stats","metadata":{}},{"id":"0b5a9dfb-55db-4e1e-baa6-491857d7d83d","cell_type":"code","source":"temp_dataset = SSLDataset(\n    data_dir = \"/kaggle/input/datasets/apollo2506/eurosat-dataset/EuroSATallBands\", \n    split_path= \"/kaggle/working/eurosat_all_bands_split.csv\",\n    split = \"all\",\n)\nmean, std = calculate_stats(temp_dataset, n_samples=50)\nprint(mean, std )","metadata":{"execution":{"iopub.execute_input":"2026-02-14T10:48:28.145464Z","iopub.status.busy":"2026-02-14T10:48:28.145196Z","iopub.status.idle":"2026-02-14T10:48:28.192596Z","shell.execute_reply":"2026-02-14T10:48:28.191918Z","shell.execute_reply.started":"2026-02-14T10:48:28.145439Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"0716eda6-1694-4829-8b66-c6b6fd4369a7","cell_type":"code","source":"# based on 10k samples\nmean= [1333.8029, 1488.1448, 1745.9066, 1985.6210, 2322.0129, 2837.1787,\n        3065.8462, 3192.4492, 3225.1826, 3344.8479,    0.0000, 2683.2991,\n        2116.8357]\nstd = [384.9683, 472.5244, 497.7275, 590.9384, 578.0192, 641.7764, 699.6282,\n        752.0769, 709.3992, 752.4539,   0.0000, 568.3574, 542.2833]\n\n\n# to avoid 0 std\nstd = [max(s, 1e-5) for s in std]   \n\n# define transform\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"186dd8fb-e8e9-4fc1-aa47-8da54f22c9fb","cell_type":"code","source":"def run_ssl(\n    data_dir,\n    split_path,\n    split=\"all\",\n    model=\"resnet18\",\n    weights=None,\n    in_channels=13,\n    mean=[],\n    std=[],\n    transform=None,\n    batch_size=64,\n    num_workers=4,\n    target_size=224,\n    lr=1e-4,\n    memory_bank_size=2048,\n    temperature=0.15,\n    use_peft=False,\n    augmentation1=None,\n    augmentation2=None,\n    max_epochs=10,\n    experiment_name= None,\n    seed=42\n):\n    \"\"\"\n    Run SSL experiment.\n\n    Args:\n        split: \"all\" for full dataset or \"subst1\"/\"subst2\"/... for sequential streaming\n        weights: ResNet18_Weights object\n        transform: preprocessing/normalization transform\n        augmentation1/2: MoCo augmentations\n        use_peft: bool, freeze backbone except last block\n        Other args: training hyperparameters\n    \"\"\"\n\n    # -----------------------------\n    # Dataset / DataLoader\n    # -----------------------------\n\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    logger = CSVLogger(\"logs\", name=f\"{experiment_name}/metrics_{timestamp}\")\n\n    transform = transforms.Compose([\n            transforms.Resize((target_size, target_size)),\n            transforms.Normalize(mean=mean, std=std)\n        ])\n    \n    dataset = SSLDataset(\n        data_dir=data_dir,\n        split_path=split_path,\n        split=split,\n        transforms=transform\n    )\n    print(f\"Dataset split '{split}' size:\", len(dataset))\n\n    data_loader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        pin_memory=True,\n        num_workers=num_workers,\n        worker_init_fn=lambda worker_id: np.random.seed(seed + worker_id)\n    )\n\n    num_batches = len(data_loader)\n    print(\"Number of batches:\", num_batches)\n\n    # -----------------------------\n    # Initialize MoCo task\n    # -----------------------------\n    task = MoCoTask(\n        model=model,\n        weights=weights,\n        in_channels= in_channels,#weights.meta['in_chans'] if weights else 3,\n        version=2,\n        size=target_size,\n        augmentation1=augmentation1,\n        augmentation2=augmentation2,\n        lr=lr,\n        memory_bank_size=memory_bank_size,\n        temperature=temperature\n    )\n\n    # -----------------------------\n    # PEFT / Full Fine-Tuning Logic\n    # -----------------------------\n    if use_peft:\n        print(\"Using PEFT: freezing backbone except last block, training projection head...\")\n        for name, param in task.backbone.named_parameters():\n            param.requires_grad = \"layer4\" in name\n    else:\n        print(\"Full fine-tuning: backbone and projection head trainable...\")\n        for param in task.backbone.parameters():\n            param.requires_grad = True\n\n    # Momentum backbone always frozen\n    for param in task.backbone_momentum.parameters():\n        param.requires_grad = False\n\n    # Projection head always trainable\n    for param in task.projection_head.parameters():\n        param.requires_grad = True\n\n    summary_trainable(task)\n\n    # -----------------------------\n    # Trainer\n    # -----------------------------\n    trainer = Trainer(\n        max_epochs=max_epochs,\n        enable_progress_bar=True,\n        log_every_n_steps=num_batches,\n        precision=32,\n        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n        deterministic=True,\n        logger=logger\n    )\n\n    # -----------------------------\n    # Training\n    # -----------------------------\n    start_time = time.time()\n    trainer.fit(task, data_loader)\n    end_time = time.time()\n    print(f\"Training time: {(end_time-start_time)/60:.2f} min\")\n\n    print(task.trainer.logged_metrics)\n\n    # -----------------------------\n    # Save checkpoint / weights\n    # -----------------------------\n    os.makedirs(\"checkpoints\", exist_ok=True)\n    torch.save(task.backbone.state_dict(), f\"{experiment_name}/checkpoints/ssl_backbone_{timestamp}.pth\")\n    torch.save(task.projection_head.state_dict(), f\"{experiment_name}/checkpoints/projection_head_{timestamp}.pth\")\n    trainer.save_checkpoint(f\"{experiment_name}/checkpoints/ssl_ckpt_{timestamp}.ckpt\")\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}